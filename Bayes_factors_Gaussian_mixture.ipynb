{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import iqr\n",
    "\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import pymbar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 428\n",
    "SIZE = 100\n",
    "\n",
    "SMC_STEPS = 20000\n",
    "MET_STEPS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture(mu1=0, sigma1=1,\n",
    "                     mu2=5, sigma2=2,\n",
    "                     rho=0.5,\n",
    "                     size=100, random_state=None):\n",
    "    rnd = np.random.RandomState(random_state)\n",
    "    x = []\n",
    "    for _ in range(size):\n",
    "        if rnd.rand() < rho:\n",
    "            x.append(rnd.normal(loc=mu1, scale=sigma1))\n",
    "        else:\n",
    "            x.append(rnd.normal(loc=mu2, scale=sigma2))\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gaussian_mixture(rho=0.5, size=SIZE, random_state=SEED)\n",
    "sns.distplot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "Fit to one Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_1:\n",
    "    mu = pm.Uniform(\"mu\", lower=-10, upper=10)\n",
    "    sigma = pm.Uniform(\"sigma\", lower=0, upper=5)\n",
    "    \n",
    "    obs = pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=x)\n",
    "\n",
    "print(model_1.vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "Two Gaussians with separate mu and sigma. Fix rho at 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_2:\n",
    "    w = pm.floatX([.5, .5])\n",
    "    mu1 = pm.Uniform(\"mu1\", lower=-10, upper=10)\n",
    "    #dmu = pm.Uniform(\"dmu\", lower=0, upper=10)\n",
    "    dmu = pm.Uniform(\"dmu\", lower=3, upper=6)\n",
    "    \n",
    "    sigma1 = pm.Uniform(\"sigma1\", lower=0, upper=5)\n",
    "    #sigma2 = pm.Uniform(\"sigma2\", lower=0, upper=5)\n",
    "    sigma2 = pm.Uniform(\"sigma2\", lower=1, upper=3)\n",
    "    \n",
    "    means = tt.zeros([2])\n",
    "    means = tt.set_subtensor(means[0], mu1)\n",
    "    means = tt.set_subtensor(means[1], mu1 + dmu)\n",
    "    \n",
    "    sigmas = tt.zeros([2])\n",
    "    sigmas = tt.set_subtensor(sigmas[0], sigma1)\n",
    "    sigmas = tt.set_subtensor(sigmas[1], sigma2)\n",
    "    \n",
    "    obs = pm.NormalMixture(\"obs\", w=w, mu=means, sigma=sigmas, observed=x)\n",
    "\n",
    "print(model_2.vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "Use separate `mu` and `sigma` for both Gaussians. rho uniformly distributed [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_3:\n",
    "    rho = pm.Uniform(\"rho\", lower=0, upper=1)\n",
    "    \n",
    "    mu1 = pm.Uniform(\"mu1\", lower=-10, upper=10)\n",
    "    #dmu = pm.Uniform(\"dmu\", lower=0, upper=10)\n",
    "    dmu = pm.Uniform(\"dmu\", lower=3, upper=6)\n",
    "    \n",
    "    sigma1 = pm.Uniform(\"sigma1\", lower=0, upper=5)\n",
    "    #sigma2 = pm.Uniform(\"sigma2\", lower=0, upper=5)\n",
    "    sigma2 = pm.Uniform(\"sigma2\", lower=1, upper=3)\n",
    "    \n",
    "    w = tt.zeros([2])\n",
    "    w = tt.set_subtensor(w[0], rho)\n",
    "    w = tt.set_subtensor(w[1], 1 - rho)\n",
    "    \n",
    "    means = tt.zeros([2])\n",
    "    means = tt.set_subtensor(means[0], mu1)\n",
    "    means = tt.set_subtensor(means[1], mu1 + dmu)\n",
    "    \n",
    "    sigmas = tt.zeros([2])\n",
    "    sigmas = tt.set_subtensor(sigmas[0], sigma1)\n",
    "    sigmas = tt.set_subtensor(sigmas[1], sigma2)\n",
    "    \n",
    "    obs = pm.NormalMixture(\"obs\", w=w, mu=means, sigma=sigmas, observed=x)\n",
    "\n",
    "print(model_3.vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Bayes factors using sequential MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_1:\n",
    "    tr_smc_m1 = pm.sample(SMC_STEPS, step=pm.SMC(), random_seed=SEED)\n",
    "\n",
    "pickle.dump(model_1, open(\"data/model_1.pickle\", \"wb\"))\n",
    "pickle.dump(tr_smc_m1, open(\"data/trace_smc_m1.pickle\", \"wb\"))\n",
    "\n",
    "mllh_m1 = model_1.marginal_likelihood\n",
    "print(\"mllh_m1\", mllh_m1)\n",
    "pm.traceplot(tr_smc_m1[::10])\n",
    "\n",
    "del model_1, mllh_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_2:\n",
    "    tr_smc_m2 = pm.sample(SMC_STEPS, step=pm.SMC(), random_seed=SEED)\n",
    "\n",
    "pickle.dump(model_2, open(\"data/model_2.pickle\", \"wb\"))\n",
    "pickle.dump(tr_smc_m2, open(\"data/trace_smc_m2.pickle\", \"wb\"))\n",
    "\n",
    "mllh_m2 = model_2.marginal_likelihood\n",
    "print(\"mllh_m2\", mllh_m2)\n",
    "pm.traceplot(tr_smc_m2[::10])\n",
    "\n",
    "del model_2, mllh_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_3:\n",
    "    tr_smc_m3 = pm.sample(SMC_STEPS, step=pm.SMC(), random_seed=SEED)\n",
    "\n",
    "pickle.dump(model_3, open(\"data/model_3.pickle\", \"wb\"))\n",
    "pickle.dump(tr_smc_m3, open(\"data/trace_smc_m3.pickle\", \"wb\"))\n",
    "\n",
    "mllh_m3 = model_3.marginal_likelihood\n",
    "print(\"mllh_m3\", mllh_m3)\n",
    "pm.traceplot(tr_smc_m3[::10])\n",
    "\n",
    "del model_3, mllh_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = pickle.load(open(\"data/model_1.pickle\", \"rb\"))\n",
    "mllh_m1 = model_1.marginal_likelihood\n",
    "print(\"Log10 Marginal likelihood of model_1: %0.5f\" % np.log10(mllh_m1))\n",
    "\n",
    "model_2 = pickle.load(open(\"data/model_2.pickle\", \"rb\"))\n",
    "mllh_m2 = model_2.marginal_likelihood\n",
    "print(\"Log10 Marginal likelihood of model_2: %0.5f\" % np.log10(mllh_m2))\n",
    "\n",
    "model_3 = pickle.load(open(\"data/model_3.pickle\", \"rb\"))\n",
    "mllh_m3 = model_3.marginal_likelihood\n",
    "print(\"Log10 Marginal likelihood of model_3: %0.5f\" % np.log10(mllh_m3))\n",
    "\n",
    "del model_1, model_2, model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Log10 Bayes factor, model_m2 over model_m1: %0.5f\" %(np.log10(mllh_m2) - np.log10(mllh_m1)))\n",
    "print(\"Log10 Bayes factor, model_m3 over model_m1: %0.5f\" %(np.log10(mllh_m3) - np.log10(mllh_m1)))\n",
    "print(\"Log10 Bayes factor, model_m3 over model_m2: %0.5f\" %(np.log10(mllh_m3) - np.log10(mllh_m2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAR estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(x, [5, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_from_iqr(data):\n",
    "    return iqr(data) / 1.35\n",
    "\n",
    "\n",
    "def fit_uniform(x, conf_level=90):\n",
    "    alpha = (100. - conf_level) / 2.\n",
    "    lower, upper = np.percentile(x, [alpha, conf_level + alpha])\n",
    "    res = {\"lower\": lower, \"upper\": upper}\n",
    "    return res\n",
    "\n",
    "\n",
    "def fit_uniform_trace(trace_values, conf_level=90):\n",
    "    res = {varname: fit_uniform(trace_values[varname], conf_level=conf_level) \n",
    "           for varname in trace_values}\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_uniform_pdf(lower, upper, y):\n",
    "    logp = np.zeros_like(y)\n",
    "    logp[:] = -np.inf\n",
    "    logp[(y >= lower) & (y <= upper)] = np.log(1. / (upper - lower) )\n",
    "    return logp\n",
    "\n",
    "\n",
    "def log_uniform_trace(trace_val, lower_upper_dict):\n",
    "    keys = list(trace_val.keys())\n",
    "    k0 = keys[0]\n",
    "    for k in keys[1:]:\n",
    "        assert len(trace_val[k0]) == len(trace_val[k]), k0 + \" and \" + k + \" do not have same len.\"\n",
    "    \n",
    "    nsamples = len(trace_val[k0])\n",
    "    logp = np.zeros(nsamples, dtype=float)\n",
    "    for k in keys:\n",
    "        lower = mu_sigma_dict[k][\"lower\"]\n",
    "        upper = mu_sigma_dict[k][\"upper\"]\n",
    "        y = trace_val[k]\n",
    "        logp += log_uniform_pdf(lower, upper, y)\n",
    "\n",
    "    return logp\n",
    "\n",
    "\n",
    "def draw_uniform_samples(lower_upper_dict, nsamples, random_state=None):\n",
    "    rand = np.random.RandomState(random_state)\n",
    "    keys = mu_sigma_dict.keys()\n",
    "    samples = {k: rand.uniform(low=lower_upper_dict[k][\"lower\"], \n",
    "                               high=lower_upper_dict[k][\"upper\"], \n",
    "                               size=nsamples)\n",
    "               for k in keys}\n",
    "    return samples\n",
    "\n",
    "\n",
    "def fit_normal(x, sigma_robust=False):\n",
    "    mu, sigma = norm.fit(x)\n",
    "    if sigma_robust:\n",
    "        sigma = std_from_iqr(x)\n",
    "    res = {\"mu\": mu, \"sigma\": sigma}\n",
    "    return res\n",
    "\n",
    "\n",
    "def fit_normal_trace(trace_values, sigma_robust=False):\n",
    "    res = {varname: fit_normal(trace_values[varname], sigma_robust=sigma_robust) for varname in trace_values}\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_normal_pdf(mu, sigma, y):\n",
    "    sigma2 = sigma * sigma\n",
    "    res = - 0.5 * np.log(2 * np.pi * sigma2) - (0.5 / sigma2) * (y - mu) ** 2\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_normal_trace(trace_val, mu_sigma_dict):\n",
    "    \"\"\"\n",
    "    :param trace_val: dict: varname --> ndarray\n",
    "    :param mu_sigma_dict: dict: varname --> dict: {\"mu\", \"sigma\"} -> {float, float}\n",
    "    :return: ndarray\n",
    "    \"\"\"\n",
    "    keys = list(trace_val.keys())\n",
    "    k0 = keys[0]\n",
    "    for k in keys[1:]:\n",
    "        assert len(trace_val[k0]) == len(trace_val[k]), k0 + \" and \" + k + \" do not have same len.\"\n",
    "\n",
    "    nsamples = len(trace_val[k0])\n",
    "    logp = np.zeros(nsamples, dtype=float)\n",
    "    for k in keys:\n",
    "        mu = mu_sigma_dict[k][\"mu\"]\n",
    "        sigma = mu_sigma_dict[k][\"sigma\"]\n",
    "        y = trace_val[k]\n",
    "        logp += log_normal_pdf(mu, sigma, y)\n",
    "\n",
    "    return logp\n",
    "\n",
    "\n",
    "def draw_normal_samples(mu_sigma_dict, nsamples, random_state=None):\n",
    "    rand = np.random.RandomState(random_state)\n",
    "    keys = mu_sigma_dict.keys()\n",
    "    samples = {k: rand.normal(loc=mu_sigma_dict[k][\"mu\"], scale=mu_sigma_dict[k][\"sigma\"], size=nsamples)\n",
    "               for k in keys}\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_list(dict_of_list):\n",
    "    \"\"\"\n",
    "    :param dict_of_list: dict: varname --> ndarray\n",
    "    :return: list of dic: [ {varname: float, ...}, ...  ]\n",
    "    \"\"\"\n",
    "    keys = list(dict_of_list.keys())\n",
    "    key0 = keys[0]\n",
    "    for key in keys[1:]:\n",
    "        assert len(dict_of_list[key0]) == len(dict_of_list[key]), key0 + \" and \" + key + \" do not have same len.\"\n",
    "\n",
    "    n = len(dict_of_list[key0])\n",
    "    ls_of_dic = []\n",
    "    for i in range(n):\n",
    "        dic = {key: dict_of_list[key][i] for key in keys}\n",
    "        ls_of_dic.append(dic)\n",
    "    return ls_of_dic\n",
    "\n",
    "\n",
    "def get_values_from_trace(model, trace, burn=0):\n",
    "    varnames = [var.name for var in model.vars]\n",
    "    trace_values = {var: trace.get_values(var, burn=burn) for var in varnames}\n",
    "    return trace_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior_trace(model, trace_values):\n",
    "    model_vars = set([var.name for var in model.vars])\n",
    "    trace_vars = set(trace_values.keys())\n",
    "    if model_vars != trace_vars:\n",
    "        print(\"model_vars:\", model_vars)\n",
    "        print(\"trace_vars:\", trace_vars)\n",
    "        raise ValueError(\"model_vars and trace_vars are not the same set\")\n",
    "\n",
    "    trace_values = dict_to_list(trace_values)\n",
    "    get_logp = np.vectorize(model.logp)\n",
    "    logp = get_logp(trace_values)\n",
    "    return logp\n",
    "\n",
    "\n",
    "def pot_ener(sample, model):\n",
    "    u = -log_posterior_trace(model, sample)\n",
    "    return u\n",
    "\n",
    "\n",
    "def pot_ener_normal_aug(sample, model, sample_aug, mu_sigma):\n",
    "    u1 = -log_posterior_trace(model, sample)\n",
    "    u2 = -log_normal_trace(sample_aug, mu_sigma)\n",
    "    u = u1 + u2\n",
    "    return u\n",
    "\n",
    "\n",
    "def pot_ener_uniform_aug(sample, model, sample_aug, lower_upper):\n",
    "    u1 = -log_posterior_trace(model, sample)\n",
    "    u2 = -log_uniform_trace(sample_aug, lower_upper)\n",
    "    u = u1 + u2\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_starts_with(start_str, list_of_strs):\n",
    "    \"\"\"\n",
    "    :param start_str: str\n",
    "    :param list_of_strs: list\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    found = [s for s in list_of_strs if s.startswith(start_str)]\n",
    "    if len(found) == 0:\n",
    "        raise ValueError(\"Found none\")\n",
    "    if len(found) > 1:\n",
    "        raise ValueError(\"Found many: \" + \", \".join(found))\n",
    "    return found[0]\n",
    "\n",
    "def split_complex_vars(sample_complex, vars_simple, split_type):\n",
    "    \"\"\"\n",
    "    split set of more complex vars to be used for simpler model\n",
    "    :param sample_complex: dict: varname --> ndarray, samples drawn from more complex model\n",
    "    :param vars_simple: list of str, names of vars in simple model\n",
    "    :param split_type: str, either or \"m2_for_m1\" or \"m3_for_m1\" or \"m3_for_m2\"\n",
    "    :return: (sample_main, sample_aug), (dict: varname --> ndarray, dict: varname --> ndarray)\n",
    "    \"\"\"\n",
    "    assert split_type in [\"m2_for_m1\", \"m3_for_m1\", \"m3_for_m2\"], \"Unknown split_type:\" + split_type\n",
    "\n",
    "    vars_complex = list(sample_complex.keys())\n",
    "    \n",
    "    if split_type in [\"m2_for_m1\", \"m3_for_m1\"]:\n",
    "        s_c_pairs = ((\"mu\", \"mu1\"), (\"sigma\", \"sigma1\"))\n",
    "        sample_main = {}\n",
    "        for pre_s, pre_c in s_c_pairs:\n",
    "            var_s = element_starts_with(pre_s, vars_simple)\n",
    "            var_c = element_starts_with(pre_c, vars_complex)\n",
    "            sample_main[var_s] = sample_complex[var_c]\n",
    "        \n",
    "        if split_type == \"m2_for_m1\":\n",
    "            aug_pres = [\"dmu\", \"sigma2\"]\n",
    "        else:\n",
    "            aug_pres = [\"dmu\", \"sigma2\", \"rho\"]\n",
    "            \n",
    "        sample_aug = {}\n",
    "        for pre_c in aug_pres:\n",
    "            var_c = element_starts_with(pre_c, vars_complex)\n",
    "            sample_aug[var_c] = sample_complex[var_c]\n",
    "        return sample_main, sample_aug\n",
    "\n",
    "\n",
    "    if split_type == \"m3_for_m2\":\n",
    "        common_vars = [var for var in sample_complex.keys() if var != \"rho_interval__\"]\n",
    "        sample_main = {var: sample_complex[var] for var in common_vars}\n",
    "\n",
    "        agu_vars = [\"rho_interval__\"]\n",
    "        sample_aug = {var: sample_complex[var] for var in agu_vars}\n",
    "\n",
    "        return sample_main, sample_aug\n",
    "\n",
    "\n",
    "def augment_simpler_vars(sample_simpler, fited_stats_complex, \n",
    "                         aug_type,\n",
    "                         draw_from=\"normal\",\n",
    "                         random_state=None):\n",
    "    assert aug_type in [\"m1_for_m2\", \"m1_for_m3\", \"m2_for_m3\"], \"Unknown aug_type:\" + aug_type\n",
    "    assert draw_from in [\"normal\", \"uniform\"], \"Unknown draw_from: \" + draw_from\n",
    "\n",
    "    # make sure we get correct mu_sigma_complex\n",
    "    if aug_type == \"m1_for_m2\":\n",
    "        assert \"dmu_interval__\" in mu_sigma_complex, \"dmu_interval__ not in mu_sigma_complex\"\n",
    "        assert \"rho_interval__\" not in mu_sigma_complex, \"rho_interval__ in mu_sigma_complex\"\n",
    "\n",
    "    if aug_type in [\"m1_for_m3\", \"m2_for_m3\"]:\n",
    "        assert \"rho_interval__\" in mu_sigma_complex, \"rho_interval__ not in mu_sigma_complex\"\n",
    "\n",
    "    vars_simple = list(sample_simpler.keys())\n",
    "    vars_complex = list(mu_sigma_complex.keys())\n",
    "    nsamples = len(sample_simpler[vars_simple[0]])\n",
    "    \n",
    "    if aug_type in [\"m1_for_m2\", \"m1_for_m3\"]:\n",
    "        c_s_pairs = ((\"mu1\", \"mu\"), (\"sigma1\", \"sigma\"))\n",
    "        \n",
    "        sample_main = {}\n",
    "        for pre_c, pre_s in c_s_pairs:\n",
    "            var_c = element_starts_with(pre_c, vars_complex)\n",
    "            var_s = element_starts_with(pre_s, vars_simple)\n",
    "            sample_main[var_c] = sample_simpler[var_s]\n",
    "        \n",
    "        if aug_type == \"m1_for_m2\":\n",
    "            aug_pres = [\"dmu\", \"sigma2\"]\n",
    "        else:\n",
    "            aug_pres = [\"dmu\", \"sigma2\", \"rho\"]\n",
    "        \n",
    "        stats_aug = {}\n",
    "        for pre_c in aug_pres:\n",
    "            var_c = element_starts_with(pre_c, vars_complex)\n",
    "            stats_aug[var_c] = fited_stats_complex[var_c]\n",
    "        \n",
    "        if draw_from == \"normal\":\n",
    "            sample_aug = draw_normal_samples(stats_aug, nsamples, random_state=random_state)\n",
    "        else:\n",
    "            sample_aug = draw_uniform_samples(stats_aug, nsamples, random_state=random_state)\n",
    "        \n",
    "        return sample_main, sample_aug\n",
    "\n",
    "    if aug_type == \"m2_for_m3\":\n",
    "        common_vars = [var for var in sample_simpler.keys() if var != \"rho_interval__\"]\n",
    "        sample_main = {var: sample_simpler[var] for var in common_vars}\n",
    "\n",
    "        aug_vars = [\"rho_interval__\"]\n",
    "        stats_aug = {k: fited_stats_complex[k] for k in aug_vars}\n",
    "        \n",
    "        if draw_from == \"normal\":\n",
    "            sample_aug = draw_normal_samples(stats_aug, nsamples, random_state=random_state)\n",
    "        else:\n",
    "            sample_aug = draw_uniform_samples(stats_aug, nsamples, random_state=random_state)\n",
    "\n",
    "        return sample_main, sample_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_BAR(w_F, w_R, repeats):\n",
    "    \"\"\"\n",
    "    :param w_F: ndarray\n",
    "    :param w_R: ndarray\n",
    "    :param repeats: int\n",
    "    :return: std, float\n",
    "    \"\"\"\n",
    "    n_F = len(w_F)\n",
    "    n_R = len(w_R)\n",
    "    delta_Fs = []\n",
    "    for _ in range(repeats):\n",
    "        w_F_rand = np.random.choice(w_F, size=n_F, replace=True)\n",
    "        w_R_rand = np.random.choice(w_R, size=n_R, replace=True)\n",
    "\n",
    "        df = pymbar.BAR(w_F_rand, w_R_rand, compute_uncertainty=False, relative_tolerance=1e-6, verbose=False)\n",
    "        delta_Fs.append(df)\n",
    "\n",
    "    delta_Fs = np.asarray(delta_Fs)\n",
    "    delta_Fs = delta_Fs[~np.isnan(delta_Fs)]\n",
    "    delta_Fs = delta_Fs[~np.isinf(delta_Fs)]\n",
    "\n",
    "    return delta_Fs.std()\n",
    "\n",
    "\n",
    "def bayes_factor(model_ini, sample_ini, model_fin, sample_fin,\n",
    "                 model_ini_name, model_fin_name,\n",
    "                 draw_aug_from=\"normal\", conf_level=90,\n",
    "                 sigma_robust=False, random_state=None,\n",
    "                 bootstrap=None):\n",
    "    assert draw_aug_from in [\"normal\", \"uniform\"], \"Unknown draw_aug_from: \" + draw_aug_from\n",
    "    \n",
    "    mu_sigma_fin = fit_normal_trace(sample_fin, sigma_robust=sigma_robust)\n",
    "    print(\"mu_sigma_fin:\", mu_sigma_fin)\n",
    "    \n",
    "    lower_upper_fin = fit_uniform_trace(sample_fin, conf_level=conf_level)\n",
    "    print(\"lower_upper_fin:\", lower_upper_fin)\n",
    "\n",
    "    split_type = model_fin_name + \"_for_\" + model_ini_name\n",
    "    aug_type = model_ini_name + \"_for_\" + model_fin_name\n",
    "\n",
    "    # augment initial sample\n",
    "    sample_i_for_f, sample_ini_aug = augment_simpler_vars(sample_ini, mu_sigma_fin, aug_type,\n",
    "                                                          draw_from=draw_aug_from,\n",
    "                                                          random_state=random_state)\n",
    "    # split final sample\n",
    "    sample_f_for_i, sample_fin_aug = split_complex_vars(sample_fin, sample_ini.keys(), split_type)\n",
    "\n",
    "    # potential for sample drawn from i estimated at state i\n",
    "    print(\"Calculate u_i_i: drawn from i, estimated at i\")\n",
    "    if draw_aug_from == \"normal\":\n",
    "        print(\"Augment with Normal\")\n",
    "        u_i_i = pot_ener_normal_aug(sample_ini, model_ini, sample_ini_aug, mu_sigma_fin)\n",
    "    else:\n",
    "        print(\"Augment with Uniform\")\n",
    "        u_i_i = pot_ener_uniform_aug(sample_ini, model_ini, sample_ini_aug, lower_upper_fin)\n",
    "\n",
    "    # potential for sample drawn from i estimated at state f\n",
    "    sample_ini_comb = sample_i_for_f.copy()\n",
    "    sample_ini_comb.update(sample_ini_aug)\n",
    "    print(\"Calculate u_i_f: drawn from i, estimated at f\")\n",
    "    u_i_f = pot_ener(sample_ini_comb, model_fin)\n",
    "\n",
    "    #\n",
    "    # potential for sample drawn from f estimated at state f\n",
    "    print(\"Calculate u_f_f: drawn from f, estimated at f\")\n",
    "    u_f_f = pot_ener(sample_fin, model_fin)\n",
    "\n",
    "    # potential for sample drawn from f estimated at state i\n",
    "    print(\"Calculate u_f_i: drawn from f, estimated at i\")\n",
    "    if draw_aug_from == \"normal\":\n",
    "        print(\"Augment with Normal\")\n",
    "        u_f_i = pot_ener_normal_aug(sample_f_for_i, model_ini, sample_fin_aug, mu_sigma_fin)\n",
    "    else:\n",
    "        print(\"Augment with Uniform\")\n",
    "        u_f_i = pot_ener_uniform_aug(sample_f_for_i, model_ini, sample_fin_aug, lower_upper_fin)\n",
    "\n",
    "    w_F = u_i_f - u_i_i\n",
    "    w_R = u_f_i - u_f_f\n",
    "\n",
    "    delta_F = pymbar.BAR(w_F, w_R, compute_uncertainty=False, relative_tolerance=1e-12, verbose=True)\n",
    "    bf = -delta_F\n",
    "\n",
    "    if bootstrap is None:\n",
    "        print(\"log10(bf) = %0.5f\" % (bf *np.log10(np.e)))\n",
    "        return bf\n",
    "    else:\n",
    "        print(\"Running %d bootstraps to estimate error.\" % bootstrap)\n",
    "        bf_err = bootstrap_BAR(w_F, w_R, bootstrap)\n",
    "        print(\"log10(bf) = %0.5f +/- %0.5f\" % (bf * np.log10(np.e), bf_err * np.log10(np.e)))\n",
    "        return bf, bf_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_1:\n",
    "    tr_met_m1 = pm.sample(MET_STEPS, step=pm.Metropolis(), random_seed=SEED)\n",
    "\n",
    "pickle.dump(tr_met_m1, open(\"data/trace_met_m1.pickle\", \"wb\"))\n",
    "\n",
    "pm.traceplot(tr_met_m1[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_2:\n",
    "    tr_met_m2 = pm.sample(MET_STEPS, step=pm.Metropolis(), random_seed=SEED)\n",
    "\n",
    "pickle.dump(tr_met_m2, open(\"data/trace_met_m2.pickle\", \"wb\"))\n",
    "\n",
    "pm.traceplot(tr_met_m2[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_3:\n",
    "    tr_met_m3 = pm.sample(MET_STEPS, step=pm.Metropolis(), random_seed=SEED)\n",
    "\n",
    "pickle.dump(tr_met_m3, open(\"data/trace_met_m3.pickle\", \"wb\"))\n",
    "\n",
    "pm.traceplot(tr_met_m3[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_val_met_m1 = get_values_from_trace(model_1, tr_met_m1)\n",
    "tr_val_met_m2 = get_values_from_trace(model_2, tr_met_m2)\n",
    "tr_val_met_m3 = get_values_from_trace(model_3, tr_met_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tr_val_met_m1 keys:\", tr_val_met_m1.keys())\n",
    "print(\"tr_val_met_m2 keys:\", tr_val_met_m2.keys())\n",
    "print(\"tr_val_met_m3 keys:\", tr_val_met_m3.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes factors using BAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_m2_over_m1 = bayes_factor(model_1, tr_val_met_m1, model_2, tr_val_met_m2,\n",
    "                            \"m1\", \"m2\", sigma_robust=True, random_state=123, bootstrap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_m3_over_m1 = bayes_factor(model_1, tr_val_met_m1, model_3, tr_val_met_m3,\n",
    "                            \"m1\", \"m3\", sigma_robust=True, random_state=123, bootstrap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_m3_over_m2 = bayes_factor(model_2, tr_val_met_m2, model_3, tr_val_met_m3,\n",
    "                            \"m2\", \"m3\", sigma_robust=True, random_state=123, bootstrap=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
